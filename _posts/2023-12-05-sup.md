---
layout : single
title:  지도학습:분류
use_math: true
categories: Supervised_learning
---
# 1 지도학습
- 지도학습은 문제와 정답을 모두 알려주고 학습을 시키는 방법을 뜻한다. <br>

# 2 여러가지 지도학습 알고리즘들
- 지도학습의 알고리즘은 크게 분류와 회귀로 나뉜다.<br>
- Categorical data에는 분류를, Continuous data에는 회귀를 사용한다.<br>
- 분류기법에는 KNN, Trees, Logistic Regression, Naive- Bayes, SVM등이 있다.<br>
- 회귀기법에는 선형회귀, 다항회귀 등이 있고, 또 연속형 데이터에 대해 Decision Trees나 Random Forest등의 기법도 사용한다. <br>

## 2-1. 분류(classification)
- 분류분석의 목적은 기존 데이터의 학습을 통해 카테고리를 분류해 내는 것이다.<br>

#### 2-1-1. KNN
- KNN은 기초적이지만 중요한 분류기법 중 하나이다. <br>
- 패턴 인식, 데이터마이닝 등에 사용된다.<br>
<br>
- KNN은 새로운 데이터(설명변수 값)에 대해 이와 가장 거리가 가까운 k개의 자료를 이용하여 다수결로 분류하는 알고리즘이다.<br>
->거리를 기반으로 작동하는 알고리즘이므로 정규화등의 과정을 먼저 거쳐야 한다.<br>
<br>
- 만약 다수결 투표결과 동률이면 랜덤하게 할당하거나 가장 가까운 이웃에 따라 할당하는 방식을 택한다.<br>
->k가 짝수일때 동률이 되므로 일반적으로 k는 홀수를 사용한다.<br>
<br>
![png](https://drive.google.com/uc?id=1acW-pgn2huJVEciU7r1gqWrteH_iV5jf) <br>
*출처: https://www.analyticsvidhya.com/blog/2021/01/a-quick-introduction-to-k-nearest-neighbor-knn-classification-using-python/*<br>
<br>
- 과거 자료를 이용하여 미리 분류모형을 수립하는 것이 아니라, 과거 데이터를 저장만 해두고 필요시 비교를 수행하는 lazy알고리즘이다. <br>
<br>
- KNN에서는 K의 값을 설정하는 것이 매우 중요하다.<br>
->K값이 작으면 이상치의 영향을 크게 받아서 overfitting(과적합)의 우려가 있다.<br>
*과적합:너무 세밀하게 학습 데이터 하나하나를 다 설명하려고 하다보니 정작 중요한 패턴을 설명할 수 없게 되는 현상<br>
<br>
->K값이 크면 미세한 경계부분을 잘못 분류해 underfitting의 우려가 있다.<br>
<br>
<br>
KNN의 간단한 예제이다.<br>

```python
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3) #k = 3
training_points = [
  [0.5, 0.2, 0.1],
  [0.9, 0.7, 0.3],
  [0.4, 0.5, 0.7]
]

training_labels = [0, 1, 1]

classifier.fit(training_points, training_labels)
```




<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>




```python
unknown_points = [
  [0.2, 0.1, 0.7],
  [0.4, 0.7, 0.6],
  [0.5, 0.8, 0.1]
]

guesses = classifier.predict(unknown_points)
print(guesses)
```
```
    [1 1 1]

```

- Majority voting(다수결)이 갖는 문제점: 기본적으로 어떤 카테고리가 타 카테고리보다 많이 있다면 많이 있는 카테고리를 고를 확률이 높다.<br>
->이는 불균형데이터 처리 방법(undersampling, oversampling)등의 방법을 통해 해결할 수 있다.<br>
->데이터 전처리 과정을 거치지 않아도 거리에 따라 가중치를 부여하는 방식으로 문제를 해결할 수도 있다.<br>
<br>
<br>
또 k가 크면 계산이 느리다/ 매번 모든 데이터를 활용한다(효율성 문제)등의 단점도 있다.

#### 2-1-2. SVM
- SVM은 support vector machine의 줄임말이다.<Br>
- vapnik et al.(1992)의 statistical learning theory로 소개가 되었다.<Br>
- 주어진 데이터가 어느 카테고리에 속할지 판단하는 이진 선형 분류 모델이다.<br>
- 1990년대 후반부터 대표적인 분류기로 널리 사용되었고, 현재에도 문자. 이미지 등의 데이터의 분류기로 사용되고 있다.<br>
- 분류와 회귀 양쪽 모두에 사용될 수 있다.<br>
- SVM은 목표는 데이터를 정확히 분류하는 초평면(선이 될수도 있고, 면이 될수도 있고,....)을 찾는 것이다.<br>
![png](https://drive.google.com/uc?id=1w9ReEHskGEvXsE-e2UDM27ptx22GZrRO)<br>
*출처: https://hleecaster.com/ml-svm-concept/* <br>
- 그림에 나와있는 빨간점이 있는 점선부터 파란점이 있는 점선까지의 거리를 margin이라고 한다.<br>
- 또, 점선위에 있는 점들(margin의 결정에 영향을 끼침)을 서포트 벡터라고 한다.(분류 초평면과 가장 가까운 점들)<br>
=> margin이 최대화가 될 수 있는 초평면을 찾는다.<br>
<br>

svm의 간단한 예제이다.<br>

```python
from sklearn.svm import SVC

classifier = SVC(kernel = 'linear')

training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
labels = [1, 1, 1, 0, 0, 0]

classifier.fit(training_points, labels) 
```




<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>



```python
print(classifier.predict([[3, 2]]))
```

    [1]
  

<br>
**SVR(Support Vector Regression**<br>
- 회귀를 위한 서포트벡터 머신<br>
- 하드 마진(hard margin)을 주로 사용한다.<br>
*하드 마진<br>
:아웃라이어를 허용하지 않는다. <br>
:개별적인 학습 데이터들을 다 놓치지 않으려고 아웃라이어를 허용하지 않는 기준으로, 오버피팅(overfitting) 문제가 발생할 수 있다.<br>

```python
import matplotlib.pyplot as plot
import numpy as np
import math

x = np.random.rand(100,1)
x = x * 10-5

y = np.array([math.sin(i) for i in x])
#평균 0 표준편차 1인 가우시안 정규 분포 
y = y + np.random.randn(100)
#서포트 백터 머신 모듈 가져오기 
from sklearn.svm import SVR
model = SVR()
model.fit(x,y)
relation_square = model.score(x, y)
print('결정계수 R :', relation_square)
y_p = model.predict(x)

plot.scatter(x, y, marker = '+')
plot.scatter(x, y_p, marker = 'o')
plot.show()
```

    결정계수 R : 0.46359832540337753
    


    
![png](https://drive.google.com/uc?id=1QZndFQep9sjgonaBIane3-P9uD-JhAm7)
    


<br>
<br>
**SVC(Support Vector Classification)**<br>
- 분류를 위한 서포트벡터 머신<br>
- 소프트 마진(soft margin)을 주로 사용한다.<br>
*소프트 마진<br>
:아웃라이어를 일부 허용한다. <br>
:너무 대충대충 학습하는 꼴이라 언더피팅(underfitting) 문제가 발생할 수 있다.<br>
SVC의 예제이다.<br>

```python
import pandas as pd
import numpy as np 

from sklearn.svm import SVC, SVR
from sklearn.datasets import load_iris 

# 분류
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = [iris.target_names[x] for x in iris.target]

species_to_labels = dict(zip(df['species'].unique(), range(len(df['species'].unique()))))
df['species'] = df['species'].map(species_to_labels) # 라벨을 숫자로 변환 

X = df.drop('species', axis=1)
y = df['species']

```


```python
clf = SVC(C=10, kernel='linear', random_state=100) # SVC 클래스 생성
clf.fit(X, y) ## 모형 학습

## 파라미터 추정치
print('가중치 :', clf.coef_) # 가중치는 kernel = 'linear'인 경우만 존재
print('절편항 :', clf.intercept_)
print('학습 정확도 :', np.mean(y==clf.predict(X)))

```

    가중치 : [[-0.04625854  0.5211828  -1.00304462 -0.46412978]
     [-0.00722313  0.17894121 -0.53836459 -0.29239263]
     [ 1.15034043  1.14954525 -3.53985244 -4.24622393]]
    절편항 : [ 1.4528445   1.50771313 13.63764975]
    학습 정확도 : 0.98
    

<br>
<br>
**비선형 SVM의 경우** <br>
![png](https://drive.google.com/uc?id=10Yz-U-SX_N85xaDFrrMUsZhhBtAOcuaX) <br>
*출처: https://sonsnotation.blogspot.com/2020/11/data-mining-10-2-svm-non-separable.html*<br>
- 그림에 나온것 처럼 비선형 SVM에서는 데이터의 위상을 변화시켜서(kernal 함수 사용) 초평면에 의해 분류가 가능하도록 한다.<br>
- 2차원의 점으로 나타내는 데이터를 다항식 커널은 3차원으로, RBF커널은 무한한 차원으로 변환한다.<br>
<br>

#### 2-1-3. Naive Bayes
**베이즈 정리(Bayes' theorem)** <br>
$P(A|B) = \frac{P(A\,and\,B)}{P(B)}$<br>
$= \frac{P(B|A)P(A)}{P(B)}$<br>
$= \frac{P(B|A)P(A)}{P(B|A)P(A)\,+\,P(B|A^C)P(A^C)}$<br>
<br>
$P(A|B)$ - 사건 $B$가 발생한 상태에서 사건 $A$가 발생할 조건부 확률<br>
$P(B|A)$ - 사건 $A$가 발생한 상태에서 사건 $B$가 발생할 조건부 확률<br>
$P(A)$ - 사건$A$가 발생할 확률<br>
$P(B)$ - 사건$B$가 발생할 확률<br>
<Br>
<Br>
- 베이즈 정리는 새로운 사건의 확률을 계산하기 전에 이미 일어난 사건을 고려하는 것을 전제로 하는 베이즈(혹은 베이지안)통계의 근간이다.<br>
- 영국의 수학자 앨런 튜링은 이 베이즈 정리를 활용해서 2차 세계대전 독일의 에니그마 암호를 풀어냈다.<Br>
- 베이즈 정리는 머신러닝, 통계적 모델링 등에서 널리 사용된다.<br>
<br>
<br>
**Naive Bayes 분류기**<br>
- naive는 순진하다는 뜻인데, 데이터셋의 모든 특징들이 동등하고 독립적이라고 가정하기 때문에 붙었다.<br>
- 나이브 베이즈 분류기는 텍스트 분류를 위해 전통적으로 사용되는데,<br>
- 베이즈 정리에서의 B를 data, A를 class이라고 판단하면 일종의 분류기가 된다.(B라는 조건이 주어졌을때 A의 확률을 구함)<br>
$P(class|data) = \frac{P(data|class) \times P(class)}{P(data)}$<br>
- 이런 원리를 바탕으로 단어들로 이루어진 문서들을 분류된 카테고리 중 하나로 분류하면,<Br>
$P(c|w_1,w_2,...,w_N) = P(w_1,w_2,...w_N|c)P(c)/P(w_1,w_2,...,w_N)$<br>
(*c는 class, w는 word*)<br>
으로 식이 나오는데, 분모는 모두 같으므로 계산할 필요 없고, 분자부분만 계산을 하면 된다.<br>
- 이때, 분자($P(w_1,w_2,...w_N|c)$)의 계산이 매우 어렵기 때문에 w들이 모두 독립이라고 가정해서(naive), 분자를<br>
$P(w_1,w_2,...w_N|c)\times=\times P(w_1|c)P(w_2|c)...P(w_N|c)$로 고칠 수 있다.<br>
위 수식의 확률이 가장 큰 카테고리가 최적 분류 카테고리가 된다.<br>
<br>
<Br>
**나이브 베이즈 분류기의 장점**<br>
- 1950년대 이후 광범위하게 연구되고 있으며, 적절한 전처리를 거치면 SVM급의 우수한 분류 성능을 보여준다.<br>
- 데이터 스케일(양)이 커도 다른 분류 알고리즘들에 비해 탁월하게 잘 작동한다.<br>
- outlier가 있어도 별로 영향을 받지 않는다.<Br>
- 이산형, 연속형 데이터에 대해 모두 잘 작동한다.<br>
<br>
**나이브 베이즈 분류기의 단점**<br>
- 모델에서는 데이터세트의 각 feature들이 독립이라고 가정하지만, 현실에서는 상관관계가 있는 경우가 대부분이다.<br>
- 로지스틱 회귀모델 분류기나 선형 SVM같은 선형 분류기보다 훈련속도는 빠르지만 일반화 성능이 떨어진다.<br>
- 또한, 단어수가 늘어날수록 확률값이 0으로 수렴하는 경우가 있음(1보다 작은 값을 계속 곱하면 0으로 수렴)<br>
- 클래스에 한번도 등장하지 않은 값이 있다면 확률이 0이 되어서 큰 문제가 발생함=> 이 문제를 해결하기 위해 smoothing을 수행<br>
<br>
**스무딩(smoothing)**
- 분자에는 1을 더하고 분모에는 N(데이터 세트에 등장하는 고유 단어의 개수)를 더하는 방법이나,<br>
분모에 가중치 $2\alpha$, 분자에 가중치 $\alpha$를 더하는 방법(라플라스 스무딩) 등이 있다. <br>
<br>
간단한 나이브 베이즈 분류기 예제이다.<br>

```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(["첫번째 문서 테스트", "두번째 문서 테스트"])
print(vectorizer.vocabulary_)
```

    {'첫번째': 2, '문서': 1, '테스트': 3, '두번째': 0}
    


```python
counts = vectorizer.transform (["직접 첫번째 테스트 두번째 테스트"])
print(counts.toarray())
```

    [[1 0 1 2]]


<br>

#### 2-1-4. Logistic Regression
- 로지스틱 회귀(Logistic Regression)는 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고,<br>
그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다.<br>
- 독립변수로는 명목,서열,등간,비율척도로 측정된 변수가 사용되고, 종속변수로는 범주형 변수가 사용된다.<br>
(범주형 응답변수에 사용된다는 점을 제외하면 선형회귀와 유사하다.(비선형회귀이다))<br>

<Br>
![png](https://drive.google.com/uc?id=1ZyXi_TQM4F8QLF_z2glSleBEHDSB0ZKc)<br>
*출처: https://anarinsk.github.io/lie-logit_reg/*<br>
<br>
- 그림처럼 곡선으로 fitting하는 변환을 로짓(logit)변환이라고 한다.<br>
$logit(p) = \log(\frac{p}{1-p})$ <br>
- 로지스틱 회귀분석은 원래 선형회귀 분석의 종속변수 $y$대신 $logit(p)$를 사용한다.<br>
- 따라서 로지스틱 회귀분석의 공식을 확인해보면,<br>
<br>
다중회귀 분석의 공식인 $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ 와 유사하게,<br>
$logit(p) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$<br>
$=\log(\frac{p}{1-p}) =  \beta_0 + \beta_1 x_1 + \beta_2 x_2$<br>
- 여기서 $\frac{p}{1-p}$은 $\frac{사건이 발생할 확률}{사건이 발생하지 않을 확률}$로, odds(오즈)라고 한다.<br>
- 종속변수로 단순한 0과 1대신 확률을 사용하기로 했었는데, 이는 순 이분법적 회귀분석을 할 경우 종속변수 값이 연속적이지 못해 독립변수와 종속변수의 관계를 명확하게 알아낼 수 없기 때문이다.<br>
- 확률대신 오즈를 사용하는 이유는 종속변수의 값이 0보다 작거나 1보다 큰 값을 못가지기 때문이다.(확률의 성질)<br>
- 구한 로그 오즈를 시그모이드 함수에 넣어서 0~1 사이 값으로 반환한다.<br>
<br>
**로지스틱 회귀분석의 장점**<br>
- 가장 구현이 쉬운 분류 알고리즘 중 하나이다.<br>
- 데이터의 차원이 낮은 경우 과적합의 경향이 덜하다.<br>
- 데이터가 선형으로 구분될 경우 성능이 좋은 편이다.<br>
<br>
**로지스틱 회귀분석의 단점**<br>
- 데이터의 차원이 증가하면 과적합 경향이 증가한다.<br>
- 데이터가 선형적으로 구분이 잘 안되는 경우 성능이 좋지 못하다.<br>
<br>
로지스틱 회귀분석의 예제이다.<br>

```python
import pandas as pd
url = "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
passengers = pd.read_csv(url)
print(passengers.tail())
print(passengers.shape)
```

         Survived  Pclass                            Name     Sex   Age  \
    882         0       2            Rev. Juozas Montvila    male  27.0   
    883         1       1     Miss. Margaret Edith Graham  female  19.0   
    884         0       3  Miss. Catherine Helen Johnston  female   7.0   
    885         1       1            Mr. Karl Howell Behr    male  26.0   
    886         0       3              Mr. Patrick Dooley    male  32.0   
    
         Siblings/Spouses Aboard  Parents/Children Aboard   Fare  
    882                        0                        0  13.00  
    883                        0                        0  30.00  
    884                        1                        2  23.45  
    885                        0                        0  30.00  
    886                        0                        0   7.75  
    (887, 8)
    


```python
passengers['Sex'] = passengers['Sex'].map({'female':1,'male':0})
features = passengers[['Sex', 'Age', 'FirstClass', 'SecondClass']]
survival = passengers['Survived']
passengers['FirstClass'] = passengers['Pclass'].apply(lambda x: 1 if x == 1 else 0)
passengers['SecondClass'] = passengers['Pclass'].apply(lambda x: 1 if x == 2 else 0)
```


```python
print(passengers.isnull().sum(axis = 0)) # 결측치 확인
```

    Survived                   0
    Pclass                     0
    Name                       0
    Sex                        0
    Age                        0
    Siblings/Spouses Aboard    0
    Parents/Children Aboard    0
    Fare                       0
    dtype: int64
    



```python
from sklearn.model_selection import train_test_split

train_features, test_features, train_labels, test_labels = train_test_split(features, survival)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

train_features = scaler.fit_transform(train_features)
test_features = scaler.transform(test_features)
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(train_features, train_labels)
print(f'{model.score(train_features, train_labels)=}')
print(f'{model.score(test_features, test_labels)=}')
```

    model.score(train_features, train_labels)=0.7849624060150376
    model.score(test_features, test_labels)=0.8018018018018018
    


```python
import numpy as np

hwansu = np.array([0.0, 20.0, 0.0, 0.0])
jaehwan = np.array([1.0, 17.0, 1.0, 0.0])
hwisu = np.array([0.0, 32.0, 1.0, 0.0])

sample_passengers = np.array([hwansu, jaehwan, hwisu])
sample_passengers = scaler.transform(sample_passengers)
print(model.predict(sample_passengers))
```

    [0 1 1]







