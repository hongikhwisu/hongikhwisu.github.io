---
layout : single
title: 지도학습
use_math: true
---
# 1 지도학습
지도학습은 문제와 정답을 모두 알려주고 학습을 시키는 방법을 뜻한다. <br>

# 2 여러가지 지도학습 알고리즘들
지도학습의 알고리즘은 크게 분류와 회귀로 나뉜다.<br>
Categorical data에는 분류를, Continuous data에는 회귀를 사용한다.<br>
분류기법에는 KNN, Trees, Logistic Regression, Naive- Bayes, SVM등이 있다.<br>
회귀기법에는 선형회귀, 다항회귀 등이 있고, 또 연속형 데이터에 대해 Decision Trees나 Random Forest등의 기법도 사용한다. <br>

## 2-1. 분류(classification)
분류분석의 목적은 기존 데이터의 학습을 통해 카테고리를 분류해 내는 것이다.<br>

#### 2-1-1. KNN
KNN은 기초적이지만 중요한 분류기법 중 하나이다. <br>
패턴 인식, 데이터마이닝 등에 사용된다.<br>
<br>
KNN은 새로운 데이터(설명변수 값)에 대해 이와 가장 거리가 가까운 k개의 자료를 이용하여 다수결로 분류하는 알고리즘이다.<br>
->거리를 기반으로 작동하는 알고리즘이므로 정규화등의 과정을 먼저 거쳐야 한다.<br>
<br>
만약 다수결 투표결과 동률이면 랜덤하게 할당하거나 가장 가까운 이웃에 따라 할당하는 방식을 택한다.<br>
->k가 짝수일때 동률이 되므로 일반적으로 k는 홀수를 사용한다.<br>
<br>
![png](https://drive.google.com/uc?id=1acW-pgn2huJVEciU7r1gqWrteH_iV5jf) <br>
*출처: https://www.analyticsvidhya.com/blog/2021/01/a-quick-introduction-to-k-nearest-neighbor-knn-classification-using-python/*<br>
<br>
과거 자료를 이용하여 미리 분류모형을 수립하는 것이 아니라, 과거 데이터를 저장만 해두고 필요시 비교를 수행하는 lazy알고리즘이다. <br>
<br>
KNN에서는 K의 값을 설정하는 것이 매우 중요하다.<br>
->K값이 작으면 이상치의 영향을 크게 받아서 overfitting(과적합)의 우려가 있다.<br>
*과적합:너무 세밀하게 학습 데이터 하나하나를 다 설명하려고 하다보니 정작 중요한 패턴을 설명할 수 없게 되는 현상<br>
<br>
->K값이 크면 미세한 경계부분을 잘못 분류해 underfitting의 우려가 있다.<br>
<br>
<br>
KNN의 간단한 예제이다.<br>

```python
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3) #k = 3
training_points = [
  [0.5, 0.2, 0.1],
  [0.9, 0.7, 0.3],
  [0.4, 0.5, 0.7]
]

training_labels = [0, 1, 1]

classifier.fit(training_points, training_labels)
```




<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>




```python
unknown_points = [
  [0.2, 0.1, 0.7],
  [0.4, 0.7, 0.6],
  [0.5, 0.8, 0.1]
]

guesses = classifier.predict(unknown_points)
print(guesses)
```
```
    [1 1 1]

```

Majority voting(다수결)이 갖는 문제점: 기본적으로 어떤 카테고리가 타 카테고리보다 많이 있다면 많이 있는 카테고리를 고를 확률이 높다.<br>
->이는 불균형데이터 처리 방법(undersampling, oversampling)등의 방법을 통해 해결할 수 있다.<br>
->데이터 전처리 과정을 거치지 않아도 거리에 따라 가중치를 부여하는 방식으로 문제를 해결할 수도 있다.<br>
<br>
<br>
또 k가 크면 계산이 느리다/ 매번 모든 데이터를 활용한다(효율성 문제)등의 단점도 있다.

#### 2-1-2. SVM
SVM은 support vector machine의 줄임말이다.<Br>
vapnik et al.(1992)의 statistical learning theory로 소개가 되었다.<Br>
주어진 데이터가 어느 카테고리에 속할지 판단하는 이진 선형 분류 모델이다.<br>
1990년대 후반부터 대표적인 분류기로 널리 사용되었고, 현재에도 문자. 이미지 등의 데이터의 분류기로 사용되고 있다.<br>
분류와 회귀 양쪽 모두에 사용될 수 있다.<br>
SVM은 목표는 데이터를 정확히 분류하는 초평면(선이 될수도 있고, 면이 될수도 있고,....)을 찾는 것이다.<br>
![png](https://drive.google.com/uc?id=1w9ReEHskGEvXsE-e2UDM27ptx22GZrRO)<br>
*출처: https://hleecaster.com/ml-svm-concept/* <br>
그림에 나와있는 빨간점이 있는 점선부터 파란점이 있는 점선까지의 거리를 margin이라고 한다.<br>
또, 점선위에 있는 점들(margin의 결정에 영향을 끼침)을 서포트 벡터라고 한다.(분류 초평면과 가장 가까운 점들)<br>
=> margin이 최대화가 될 수 있는 초평면을 찾는다.<br>
<br>

svm의 간단한 예제이다.<br>

```python
from sklearn.svm import SVC

classifier = SVC(kernel = 'linear')

training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
labels = [1, 1, 1, 0, 0, 0]

classifier.fit(training_points, labels) 
```




<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>



```python
print(classifier.predict([[3, 2]]))
```

    [1]
  

<br>
SVR(Support Vector Regression<br>
-회귀를 위한 서포트벡터 머신<br>
-하드 마진(hard margin)을 주로 사용한다.<br>
*하드 마진<br>
:아웃라이어를 허용하지 않는다. <br>
:개별적인 학습 데이터들을 다 놓치지 않으려고 아웃라이어를 허용하지 않는 기준으로, 오버피팅(overfitting) 문제가 발생할 수 있다.<br>

```python
import matplotlib.pyplot as plot
import numpy as np
import math

x = np.random.rand(100,1)
x = x * 10-5

y = np.array([math.sin(i) for i in x])
#평균 0 표준편차 1인 가우시안 정규 분포 
y = y + np.random.randn(100)
#서포트 백터 머신 모듈 가져오기 
from sklearn.svm import SVR
model = SVR()
model.fit(x,y)
relation_square = model.score(x, y)
print('결정계수 R :', relation_square)
y_p = model.predict(x)

plot.scatter(x, y, marker = '+')
plot.scatter(x, y_p, marker = 'o')
plot.show()
```

    결정계수 R : 0.46359832540337753
    


    
![png](https://drive.google.com/uc?id=1QZndFQep9sjgonaBIane3-P9uD-JhAm7)
    


<br>
<br>
SVC(Support Vector Classification)<br>
-분류를 위한 서포트벡터 머신<br>
-소프트 마진(soft margin)을 주로 사용한다.<br>
*소프트 마진<br>
:아웃라이어를 일부 허용한다. <br>
:너무 대충대충 학습하는 꼴이라 언더피팅(underfitting) 문제가 발생할 수 있다.<br>
SVC의 예제이다.<br>

```python
import pandas as pd
import numpy as np 

from sklearn.svm import SVC, SVR
from sklearn.datasets import load_iris 

# 분류
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = [iris.target_names[x] for x in iris.target]

species_to_labels = dict(zip(df['species'].unique(), range(len(df['species'].unique()))))
df['species'] = df['species'].map(species_to_labels) # 라벨을 숫자로 변환 

X = df.drop('species', axis=1)
y = df['species']

```


```python
clf = SVC(C=10, kernel='linear', random_state=100) # SVC 클래스 생성
clf.fit(X, y) ## 모형 학습

## 파라미터 추정치
print('가중치 :', clf.coef_) # 가중치는 kernel = 'linear'인 경우만 존재
print('절편항 :', clf.intercept_)
print('학습 정확도 :', np.mean(y==clf.predict(X)))

```

    가중치 : [[-0.04625854  0.5211828  -1.00304462 -0.46412978]
     [-0.00722313  0.17894121 -0.53836459 -0.29239263]
     [ 1.15034043  1.14954525 -3.53985244 -4.24622393]]
    절편항 : [ 1.4528445   1.50771313 13.63764975]
    학습 정확도 : 0.98
    

<br>
<br>
**비선형 SVM의 경우** <br>
![png](https://drive.google.com/uc?id=10Yz-U-SX_N85xaDFrrMUsZhhBtAOcuaX) <br>
*출처: https://sonsnotation.blogspot.com/2020/11/data-mining-10-2-svm-non-separable.html*<br>
그림에 나온것 처럼 비선형 SVM에서는 데이터의 위상을 변화시켜서(kernal 함수 사용) 초평면에 의해 분류가 가능하도록 한다.<br>
2차원의 점으로 나타내는 데이터를 다항식 커널은 3차원으로, RBF커널은 무한한 차원으로 변환한다.<br>

#### 2-1-3. Naive Bayes
**베이즈 정리(Bayes' theorem)** <br>
$P(A|B) = \frac{P(A\,and\,B)}{P(B)}$<br>
$= \frac{P(B|A)P(A)}{P(B)}$<br>
$= \frac{P(B|A)P(A)}{P(B|A)P(A)\,+\,P(B|A^C)P(A^C)}$<br>
<br>
$P(A|B)$ - 사건 $B$가 발생한 상태에서 사건 $A$가 발생할 조건부 확률<br>
$P(B|A)$ - 사건 $A$가 발생한 상태에서 사건 $B$가 발생할 조건부 확률<br>
$P(A)$ - 사건$A$가 발생할 확률<br>
$P(B)$ - 사건$B$가 발생할 확률<br>
<Br>
<Br>
베이즈 정리는 새로운 사건의 확률을 계산하기 전에 이미 일어난 사건을 고려하는 것을 전제로 하는 베이즈(혹은 베이지안)통계의 근간이다.<br>
영국의 수학자 앨런 튜링은 이 베이즈 정리를 활용해서 2차 세계대전 독일의 에니그마 암호를 풀어내었다.<Br>
베이즈 정리는 머신러닝, 통계적 모델링 등에서 널리 사용된다.<br>
<br>
<br>
**Naive Bayes 분류기**<br>
naive는 순진하다는 뜻인데, 데이터셋의 모든 특징들이 동등하고 독립적이라고 가정하기 때문에 붙었다.<br>
나이브 베이즈 분류기는 텍스트 분류를 위해 전통적으로 사용되는데,<br>
베이즈 정리에서의 B를 데이터, A를 레이블이라고 판단하면 일종의 분류기가 된다.(B라는 조건이 주어졌을때 A의 확률을 구함)<br>
$$P(class|data) = \frac{P(data|class) \times P(class)}{P(data)}$$




