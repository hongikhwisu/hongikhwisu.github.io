---
layout : single
title:  비지도학습:차원축소
use_math: true
categories: Unsupervised_learning
---
# 1. 비지도학습
- 사람의 도움없이 컴퓨터가 스스로 데이터를 파악하고 규칙을 찾는 방법을 말한다.<br>
-> y값 없이 x값만으로 데이터를 학습하는 방법<br>
- 비지도학습 시에는 비정형 데이터를 학습한다.<br>
- 훈련 정보가 사전에 정답이 레이블 되지 않거나, 구조를 알 수 없는 데이터일때 그 데이터의 의미있는 정보를 추출한다.<br>
데이터세트에서 추론을 도출하고, 레이블이 없는 데이터에 있을 수 있는 숨겨진 구조를 표시한다.<br>

<br>
# 2. 여러가지 비지도학습 알고리즘들
- 비지도학습의 알고리즘은 군집화(clustering), 차원 축소(dimensionality reduction), 연관성 분석(association analysis)에 사용된다.<br>
- Catgorical 데이터에는 연관성 분석, Hidden Markov model이 사용되고, Continuous 데이터에는 군집화와 차원축소가 사용된다.<br>
- 차원축소에는 SVD, PCA등이 사용되고, 군집화에는 K-means, DBSCAN등이 사용된다.<br>
- 연관성분석에는 APriori, FP-Growth등이 사용된다.<br>
<br>

## 2-2. 차원축소
- 데이터의 차원을 낮추는 기법을 말한다. 차원을 낮추면 데이터 입력회수를 관리 가능한 크기로 줄이면서 데이터 세트의 무결성을 최대한 보존한다.<br>
- 일반적으로 데이터 전처리에서 사용된다.<br>

#### 2-2-1. PCA(주성분 분석)
- 가장 널리 사용되는 차원(변수) 축소 기법 중 하나<br>
- 원 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법<br>
- PCA는 기존의 변수를 조합하여 서로 연관성이 없는 새로운 변수, 즉 주성분(principal component, PC)들을 만들어 냄<br>
- 주성분의 개수를 증가시킴에 따라 원 데이터의 분산의 보존수준이 높아짐<br>
<br>

**PCA 알고리즘**<br>
1. 학습 데이터셋에서 분산이 최대인 축(axis)을 찾음<br>
2. 첫번째 축과 직교(orthogonal)하면서 분산이 최대인 두 번째 축을 찾음<br>
3. 첫 번째 축과 두 번째 축에 직교하고 분산을 최대한 보존하는 세 번째 축을 찾음<br>
4. 1~3과 같은 방법으로 데이터셋의 차원(특성 수)만큼의 축을 찾음<br>

![png](https://drive.google.com/uc?id=1iN8LzC0BWkyv2dtw9bvFdWOaa_00-N_R)<br>
*출처: https://chriver58.quarto.pub/* <br>
<br>
강의자료에 있는 PCA예제이다.(살짝 변형)<br>
```python
from sklearn import datasets
from sklearn.decomposition import PCA

# iris 데이터셋을 로드
iris = datasets.load_iris()

X = iris.data # iris 데이터셋의 피쳐들
y = iris.target # iris 데이터셋의 타겟
target_names = list(iris.target_names) # iris 데이터셋의 타겟 이름

print(f'{X.shape = }, {y.shape = }') # 150개 데이터, 4 features
print(f'{target_names = }')  
```

    X.shape = (150, 4), y.shape = (150,)
    target_names = ['setosa', 'versicolor', 'virginica']
    


```python
# PCA의 객체를 생성, 차원은 2차원으로 설정(현재는 4차원)
pca = PCA(n_components=2)

# PCA를 수행. PCA는 비지도 학습이므로 y값을 넣지 않음
pca_fitted = pca.fit(X)

print(f'{pca_fitted.components_ = }')  # 주성분 벡터
print(f'{pca_fitted.explained_variance_ratio_ = }') # 주성분 벡터의 설명할 수 있는 분산 비율

X_pca = pca_fitted.transform(X) # 주성분 벡터로 데이터를 변환
print(f'{X_pca.shape = }')  # 4차원 데이터가 2차원 데이터로 변환됨
```

    pca_fitted.components_ = array([[ 0.36138659, -0.08452251,  0.85667061,  0.3582892 ],
           [ 0.65658877,  0.73016143, -0.17337266, -0.07548102]])
    pca_fitted.explained_variance_ratio_ = array([0.92461872, 0.05306648])
    X_pca.shape = (150, 2)
    


```python
# 시각화 하기
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Seaborn을 이용하기 위해 데이터프레임으로 변환
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
y = pd.Series(y).replace({0:'setosa', 1:'versicolor', 2:'virginica'})

fig, ax = plt.subplots(1, 1, figsize=(6,3))

sns.scatterplot(df_pca, x='PC1', y='PC2', hue=y, style=y, palette='Set1')
ax.set_title('PCA of IRIS dataset')
plt.show()
```


    
![png](https://drive.google.com/uc?id=1olrY-ozt8cnKrHSdHNOlPT8wsfBG-S3B)
    

<br>
#### 2-2-2. SVD(비정칙값 분해)
- 특이값 분해(Singular Value Decomposition, SVD)는 행렬을 세 개의 행렬의 곱으로 분해하는 기법이다.<br>
- SVD는 선형 대수학에서 다양한 응용 분야에 활용되며, 특히 차원 축소, 행렬 근사, 행렬 랭크 감소 등에 사용된다.<br>


주어진 $m×n$ 크기의 행렬 $A$를 세 개의 행렬의 곱으로 분해하면<br>
$A = UΣV^T$이고,<br>
$U$는 $m×m$ 크기의 직교 행렬(orthogonal matrix), $Σ$는 $m×n$ 크기의 대각 행렬(diagonal matrix), $V^ T$는 $n×n$ 크기의 직교 행렬의 전치행렬이다.<br>
$Σ$가 대각원소로 특이값을 가진다.<br>
<br>

**차원축소 방법**<br>
차원을 축소하는 방법은 가장 작은 특이값을 0으로 만드는 것이다.<br>
->$Σ$의 특이값들은 내림차순으로 배열되어 있는데, 중요도의 순이다. <br>
따라서 가장 작은 특이값을 0으로 만든다는 것은 원본 행렬에 가장 근사하면서, 불필요한 특징을 제거한 행렬을 얻게 되는 것이다.<br>

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn import datasets
import pandas as pd
iris = datasets.load_iris() #iris데이터셋 이용
data = iris['data']
df = pd.DataFrame(data, columns = iris['feature_names'])
df['target'] = iris['target']
```


```python
from sklearn.decomposition import TruncatedSVD #scikit-learn에서는 SVD를 위해 TruncatedSVD를 사용
import matplotlib.pyplot as plt
svd = TruncatedSVD(n_components = 2)
data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])
svd_data = svd.fit_transform(data_scaled)
plt.scatter(svd_data[:,0], svd_data[:,1], c = df['target'])
```




    <matplotlib.collections.PathCollection at 0x18a10111100>




    
![png](https://drive.google.com/uc?id=11vapPOjkSd-Zkl4aKmgzTcUUsycqcNlw)
    



<br>

## 2-3. 연관성 분석
- 연관성 분석은 특정 데이터 세트에서 변수 간의 관계를 발견하기 위한 규칙 기반 학습 방법이다.<br>
