---
layout : single
title: 지도학습 - 회귀
use_math : true 
---
# 1 지도학습
- 지도학습은 문제와 정답을 모두 알려주고 학습을 시키는 방법을 뜻한다. <br>

# 2 여러가지 지도학습 알고리즘들
- 지도학습의 알고리즘은 크게 분류와 회귀로 나뉜다.<br>
- Categorical data에는 분류를, Continuous data에는 회귀를 사용한다.<br>
- 분류기법에는 KNN, Trees, Logistic Regression, Naive- Bayes, SVM등이 있다.<br>
- 회귀기법에는 선형회귀, 다항회귀 등이 있고, 또 연속형 데이터에 대해 Decision Trees나 Random Forest등의 기법도 사용한다. <br>

## 2-2.회귀
- 예측 변수와 종속변수가 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계를 찾는다.<br>
&nbsp;독립변수로는 영향을 미칠 것이라고 생각되는 변수를,<br>
&ensp;종속변수로는 영향을 받을 것이라고 생각되는 변수를 사용한다.<br>
-미래의 종속변수 값을 예측할때에도 사용된다.<br>
<br>
#### 2-2-1. 선형회귀
- 두 변수 사이의 관계를 분석하는 방법이다.<br>
$x$변수로는 등간/비율척도를 사용하고, $y$변수도 마찬가지이다.<br>
- 선형 회귀모델은 말 그대로 독립변수와 종속변수 사이의 관계를 직선으로 표현한다.<br>
(단순 선형회귀: 독립변수 1개, 다중 선형회귀: 독립변수 2개 이상)<br>
$y = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p+ \epsilon$ <br>
<br>
**선형회귀 분석의 기본 가정**<br>
- 종속변수와 독립변수 간에는 선형성이 성립한다.<br>
- 독립변수는 정확히 측정된 값으로 확률적으로 변하는 값이 아닌 고정된 값이다.<br>
- 오차는 평균이 0, 분산이 $\sigma ^2$인 정규분포를 따르며, 평균과 분산이 일정하다.<br>
- 오차들 간은 서로 독립이다.<br>
- 독립 변수들 간에는 다중 공선성이 적어야 한다.<br>
다중공선성 : 독립변수 끼리 높은 상관관계를 보이는 것을 말한다. VIF값으로 확인하고, VIF값이 10 이상이면 다중공선성이 존재할 가능성이 높다.(다중 선형 회귀 모델에서 확인)<br>
<br>
- 회귀모델에서의 종속변수값은 정확한 값이 아닌, 확률분포의 형태이다. => 각 확률분포의 평균을 이은 것은 직선으로 나타난다.<br>
- 위의 다중선형회귀모형 수식에서, 좌변의 $y$는 확률변수이고, 우변에서는 $\epsilon$이 확률 변수이다.<br>
<br>
- 잔차($e_i$)는 실제 관측값과 회귀모델 추정값의 차이를 의미한다.($e_i = y_i - \hat{y}_i$)<br>
- 위의 다중 선형 회귀 모형 공식에서 오차를 최소화하는 회귀모델의 계수를 추정해야 하는데, 이를 최소 제곱법이라고 한다.<br>

$\sum_{i=1}^{n} \epsilon ^2 = \sum_{i=1}^{n} (y_i - \beta _0 -\beta_1 x_1)^2$ 공식을 각각 a,b에 대해 편미분 하여 값을 구한다.<br>

**회귀 분석의 결정 계수**<br>
![png](https://drive.google.com/uc?id=1zohuea_HViQ898KAZwLPv3aUTu73uvkB)<br>
*출처 : https://vitalflux.com/linear-regression-explained-python-sklearn-examples/* <br>

- SST는 총 제곱합으로, $\sum_{i=1}^{n} (y_i - \bar{y})^2$로 나타낸다.<br>
(SST는 SSR과 SSE의 합이기도 하다.) <br>
- SSR은 설명되는 회귀 제곱합으로, $\sum_{i=1}^{n} (\hat{y} - \bar{y})^2$로 나타낸다.<br>
- SSE는 설명되지않는 잔차 제곱합으로, $\sum_{i=1}^{n} (y_i - \hat{y})^2$로 나타낸다.<br>
- $R^2$ 값은 결정계수로, 1에 가까울 수록 적합성이 뛰어나다.<br>
$\frac{SSR}{SST}$로 나타낸다.<br>
<br>
다음은 간단한 단순 선형 회귀 모델의 예제이다. <br>

```python
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv("heights.csv")#kaggle에서 다운받은 heights and weights dataset
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Index</th>
      <th>Height(Inches)</th>
      <th>Weight(Pounds)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>65.78331</td>
      <td>112.9925</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>71.51521</td>
      <td>136.4873</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>69.39874</td>
      <td>153.0269</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>68.21660</td>
      <td>142.3354</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>67.78781</td>
      <td>144.2971</td>
    </tr>
  </tbody>
</table>
</div>




```python
X = df["Height(Inches)"]
y = df["Weight(Pounds)"]
line_fitter = LinearRegression()
line_fitter.fit(X.values.reshape(-1,1), y)
print(f'{line_fitter.predict([[70]]) = }') # 키가 70inch인 사람의 몸무게 예측
```

    line_fitter.predict([[70]]) = array([133.26760811])
    


```python
print(f'{line_fitter.coef_ =}') # 기울기
print(f'{line_fitter.intercept_=}') # y절편
```

    line_fitter.coef_ =array([3.08347645])
    line_fitter.intercept_=-82.57574306454079
    

<br>
간단한 다중 선형 회귀 모델의 예제이다. <br>
```python
import pandas as pd

df = pd.read_csv("manhattan.csv") # kaggle의 Rent Amount of June 2016 Streeteasy 데이터
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rental_id</th>
      <th>building_id</th>
      <th>rent</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>size_sqft</th>
      <th>min_to_subway</th>
      <th>floor</th>
      <th>building_age_yrs</th>
      <th>no_fee</th>
      <th>has_roofdeck</th>
      <th>has_washer_dryer</th>
      <th>has_doorman</th>
      <th>has_elevator</th>
      <th>has_dishwasher</th>
      <th>has_patio</th>
      <th>has_gym</th>
      <th>neighborhood</th>
      <th>submarket</th>
      <th>borough</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1545</td>
      <td>44518357</td>
      <td>2550</td>
      <td>0.0</td>
      <td>1</td>
      <td>480</td>
      <td>9</td>
      <td>2.0</td>
      <td>17</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>Upper East Side</td>
      <td>All Upper East Side</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2472</td>
      <td>94441623</td>
      <td>11500</td>
      <td>2.0</td>
      <td>2</td>
      <td>2000</td>
      <td>4</td>
      <td>1.0</td>
      <td>96</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Greenwich Village</td>
      <td>All Downtown</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10234</td>
      <td>87632265</td>
      <td>3000</td>
      <td>3.0</td>
      <td>1</td>
      <td>1000</td>
      <td>4</td>
      <td>1.0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Astoria</td>
      <td>Northwest Queens</td>
      <td>Queens</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2919</td>
      <td>76909719</td>
      <td>4500</td>
      <td>1.0</td>
      <td>1</td>
      <td>916</td>
      <td>2</td>
      <td>51.0</td>
      <td>29</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>Midtown</td>
      <td>All Midtown</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2790</td>
      <td>92953520</td>
      <td>4795</td>
      <td>1.0</td>
      <td>1</td>
      <td>975</td>
      <td>3</td>
      <td>8.0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>Greenwich Village</td>
      <td>All Downtown</td>
      <td>Manhattan</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
mlr = LinearRegression()
mlr.fit(x_train, y_train) 
```




<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>




```python
my_apartment = [[1, 1, 620, 16, 1, 98, 1, 0, 1, 0, 0, 1, 1, 0]]
my_predict = mlr.predict(my_apartment)
print(f'{my_predict=}')
```

    my_predict=array([[2005.32275882]])
    

   
    


```python
print(mlr.coef_) #계수 확인
```

    [[-486.60032808 1389.89343765    4.69852898  -17.65217356   39.85448773
        -4.11190558  -85.95343096   39.9080553   141.21411307   10.50324996
       164.47431448   55.05079097 -194.76346515    8.9496282 ]]
    


```python
# 일치하는지 확인 :완전한 직선이면 일치(실제-예측 그래프)
import matplotlib.pyplot as plt
y_predict = mlr.predict(x_test)
plt.scatter(y_test, y_predict, alpha=0.4)
plt.xlabel("Actual Rent")
plt.ylabel("Predicted Rent")
plt.title("MULTIPLE LINEAR REGRESSION")
plt.show()
```


    
![png](https://drive.google.com/uc?id=1bIeQWnQdJHjHJjXTKn2vYRDIa9EjSwMz)
    



```python
print(mlr.score(x_train, y_train)) #결정계수
```

    0.7373487000930192
    

<br>
<br>
#### 2-2-2. Decision Tree
- Decision Tree(의사결정 나무)는 각 데이터들이 가진 속성들로부터 패턴을 찾아내서 분류과제를 수행할 수 있도록 하는 지도학습 머신러닝 모델이다.<br>
- 분류와 회귀 모두 가능하다.<br>
![png](https://drive.google.com/uc?id=1IxF52d6771PCT4eLU4ctkFa9LtpOhkH-) <br>
*출처: https://ratsgo.github.io/machine%20learning/2017/03/26/tree/* <br>


