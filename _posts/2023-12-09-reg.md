---
layout : single
title: 분류학습 - 회귀
use_math : true 
---
# 1 지도학습
- 지도학습은 문제와 정답을 모두 알려주고 학습을 시키는 방법을 뜻한다. <br>

# 2 여러가지 지도학습 알고리즘들
- 지도학습의 알고리즘은 크게 분류와 회귀로 나뉜다.<br>
- Categorical data에는 분류를, Continuous data에는 회귀를 사용한다.<br>
- 분류기법에는 KNN, Trees, Logistic Regression, Naive- Bayes, SVM등이 있다.<br>
- 회귀기법에는 선형회귀, 다항회귀 등이 있고, 또 연속형 데이터에 대해 Decision Trees나 Random Forest등의 기법도 사용한다. <br>

## 2-2.회귀
- 예측 변수와 종속변수가 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계를 찾는다.<br>
&nbsp;독립변수로는 영향을 미칠 것이라고 생각되는 변수를,<br>
&ensp;종속변수로는 영향을 받을 것이라고 생각되는 변수를 사용한다.<br>
-미래의 종속변수 값을 예측할때에도 사용된다.<br>
<br>
#### 2-2-1. 선형회귀
- 두 변수 사이의 관계를 분석하는 방법이다.<br>
$x$변수로는 등간/비율척도를 사용하고, $y$변수도 마찬가지이다.<br>
- 선형 회귀모델은 말 그대로 독립변수와 종속변수 사이의 관계를 직선으로 표현한다.<br>
(단순 선형회귀: 독립변수 1개, 다중 선형회귀: 독립변수 2개 이상)<br>
$y = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p+ \epsilon$ <br>
<br>
**선형회귀 분석의 기본 가정**<br>
- 종속변수와 독립변수 간에는 선형성이 성립한다.<br>
- 독립변수는 정확히 측정된 값으로 확률적으로 변하는 값이 아닌 고정된 값이다.<br>
- 오차는 평균이 0, 분산이 $\sigma ^2$인 정규분포를 따르며, 평균과 분산이 일정하다.<br>
- 오차들 간은 서로 독립이다.<br>
- 독립 변수들 간에는 다중 공선성이 적어야 한다.<br>
다중공선성 : 독립변수 끼리 높은 상관관계를 보이는 것을 말한다. VIF값으로 확인하고, VIF값이 10 이상이면 다중공선성이 존재할 가능성이 높다.(다중 선형 회귀 모델에서 확인)<br>
<br>
- 회귀모델에서의 종속변수값은 정확한 값이 아닌, 확률분포의 형태이다. => 각 확률분포의 평균을 이은 것은 직선으로 나타난다.<br>
- 위의 다중선형회귀모형 수식에서, 좌변의 $y$는 확률변수이고, 우변에서는 $\epsilon$이 확률 변수이다.<br>
<br>
- 잔차($e_i$)는 실제 관측값과 회귀모델 추정값의 차이를 의미한다.($e_i = y_i - \hat{y}_i$)<br>
- 위의 다중 선형 회귀 모형 공식에서 오차를 최소화하는 회귀모델의 계수를 추정해야 하는데, 이를 최소 제곱법이라고 한다.<br>

$\sum_{i=1}^{n} \epsilon ^2 = \sum_{i=1}^{n} (y_i - \beta _0 -\beta_1 x_1)^2$ 공식을 각각 a,b에 대해 편미분 하여 값을 구한다.<br>

**회귀 분석의 결정 계수**<br>
![png](https://drive.google.com/uc?id=1zohuea_HViQ898KAZwLPv3aUTu73uvkB)<br>
*출처 : https://vitalflux.com/linear-regression-explained-python-sklearn-examples/* <br>

- SST는 총 제곱합으로, $\sum_{i=1}^{n} (y_i - \bar{y})^2$로 나타낸다.<br>
(SST는 SSR과 SSE의 합이기도 하다.) <br>
- SSR은 설명되는 회귀 제곱합으로, $\sum_{i=1}^{n} (\hat{y} - \bar{y})^2$로 나타낸다.<br>
- SSE는 설명되지않는 잔차 제곱합으로, $\sum_{i=1}^{n} (y_i - \hat{y})^2$로 나타낸다.<br>
- $R^2$ 값은 결정계수로, 1에 가까울 수록 적합성이 뛰어나다.<br>
$\frac{SSR}{SST}$로 나타낸다.<br>
<br>
다음은 간단한 단순 선형 회귀 모델의 예제이다. <br>

```python
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv("heights.csv")#kaggle에서 다운받은 heights and weights dataset
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Index</th>
      <th>Height(Inches)</th>
      <th>Weight(Pounds)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>65.78331</td>
      <td>112.9925</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>71.51521</td>
      <td>136.4873</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>69.39874</td>
      <td>153.0269</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>68.21660</td>
      <td>142.3354</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>67.78781</td>
      <td>144.2971</td>
    </tr>
  </tbody>
</table>
</div>




```python
X = df["Height(Inches)"]
y = df["Weight(Pounds)"]
line_fitter = LinearRegression()
line_fitter.fit(X.values.reshape(-1,1), y)
print(f'{line_fitter.predict([[70]]) = }') # 키가 70inch인 사람의 몸무게 예측
```

    line_fitter.predict([[70]]) = array([133.26760811])
    


```python
print(f'{line_fitter.coef_ =}') # 기울기
print(f'{line_fitter.intercept_=}') # y절편
```

    line_fitter.coef_ =array([3.08347645])
    line_fitter.intercept_=-82.57574306454079
    

<br>
<br>
#### 2-2-2. Random Forests
