---
layout : single
title: 지도학습:회귀
use_math : true 
categories: Supervised_learning
---
# 1 지도학습
- 지도학습은 문제와 정답을 모두 알려주고 학습을 시키는 방법을 뜻한다. <br>

# 2 여러가지 지도학습 알고리즘들
- 지도학습의 알고리즘은 크게 분류와 회귀로 나뉜다.<br>
- Categorical data에는 분류를, Continuous data에는 회귀를 사용한다.<br>
- 분류기법에는 KNN, Trees, Logistic Regression, Naive- Bayes, SVM등이 있다.<br>
- 회귀기법에는 선형회귀, 다항회귀 등이 있고, 또 연속형 데이터에 대해 Decision Trees나 Random Forest등의 기법도 사용한다.
- 분류와 회귀에 모두 사용되는 방법도 여러개 있다.
<br>

## 2-2.회귀
- 예측 변수와 종속변수가 주어졌을 때 출력값을 예측하는 두 변수 사이의 관계를 찾는다.<br>
&nbsp;독립변수로는 영향을 미칠 것이라고 생각되는 변수를,<br>
&ensp;종속변수로는 영향을 받을 것이라고 생각되는 변수를 사용한다.<br>
-미래의 종속변수 값을 예측할때에도 사용된다.<br>
<br>

#### 2-2-1. 선형회귀
- 두 변수 사이의 관계를 분석하는 방법이다.<br>
$x$변수로는 등간/비율척도를 사용하고, $y$변수도 마찬가지이다.<br>
- 선형 회귀모델은 말 그대로 독립변수와 종속변수 사이의 관계를 직선으로 표현한다.<br>
(단순 선형회귀: 독립변수 1개, 다중 선형회귀: 독립변수 2개 이상)<br>
$y = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p+ \epsilon$ <br>
<br>

**선형회귀 분석의 기본 가정**<br>
- 종속변수와 독립변수 간에는 선형성이 성립한다.<br>
- 독립변수는 정확히 측정된 값으로 확률적으로 변하는 값이 아닌 고정된 값이다.<br>
- 오차는 평균이 0, 분산이 $\sigma ^2$인 정규분포를 따르며, 평균과 분산이 일정하다.<br>
- 오차들 간은 서로 독립이다.<br>
- 독립 변수들 간에는 다중 공선성이 적어야 한다.<br>

다중공선성 : 독립변수 끼리 높은 상관관계를 보이는 것을 말한다. VIF값으로 확인하고, VIF값이 10 이상이면 다중공선성이 존재할 가능성이 높다.(다중 선형 회귀 모델에서 확인)<br>
<br>
- 회귀모델에서의 종속변수값은 정확한 값이 아닌, 확률분포의 형태이다. => 각 확률분포의 평균을 이은 것은 직선으로 나타난다.<br>
- 위의 다중선형회귀모형 수식에서, 좌변의 $y$는 확률변수이고, 우변에서는 $\epsilon$이 확률 변수이다.<br>
<br>
- 잔차($e_i$)는 실제 관측값과 회귀모델 추정값의 차이를 의미한다.($e_i = y_i - \hat{y}_i$)<br>
- 위의 다중 선형 회귀 모형 공식에서 오차를 최소화하는 회귀모델의 계수를 추정해야 하는데, 이를 최소 제곱법이라고 한다.<br>

$\sum_{i=1}^{n} \epsilon ^2 = \sum_{i=1}^{n} (y_i - \beta _0 -\beta_1 x_1)^2$ 공식을 각각 a,b에 대해 편미분 하여 값을 구한다.<br>

**회귀 분석의 결정 계수**<br>
![png](https://drive.google.com/uc?id=1zohuea_HViQ898KAZwLPv3aUTu73uvkB)<br>
*출처 : https://vitalflux.com/linear-regression-explained-python-sklearn-examples/* <br>

- SST는 총 제곱합으로, $\sum_{i=1}^{n} (y_i - \bar{y})^2$로 나타낸다.<br>
(SST는 SSR과 SSE의 합이기도 하다.) <br>
- SSR은 설명되는 회귀 제곱합으로, $\sum_{i=1}^{n} (\hat{y} - \bar{y})^2$로 나타낸다.<br>
- SSE는 설명되지않는 잔차 제곱합으로, $\sum_{i=1}^{n} (y_i - \hat{y})^2$로 나타낸다.<br>
- $R^2$ 값은 결정계수로, 1에 가까울 수록 적합성이 뛰어나다.<br>
$\frac{SSR}{SST}$로 나타낸다.<br>
<br>

다음은 간단한 단순 선형 회귀 모델의 예제이다. <br>

```python
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv("heights.csv")#kaggle에서 다운받은 heights and weights dataset
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Index</th>
      <th>Height(Inches)</th>
      <th>Weight(Pounds)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>65.78331</td>
      <td>112.9925</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>71.51521</td>
      <td>136.4873</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>69.39874</td>
      <td>153.0269</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>68.21660</td>
      <td>142.3354</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>67.78781</td>
      <td>144.2971</td>
    </tr>
  </tbody>
</table>
</div>




```python
X = df["Height(Inches)"]
y = df["Weight(Pounds)"]
line_fitter = LinearRegression()
line_fitter.fit(X.values.reshape(-1,1), y)
print(f'{line_fitter.predict([[70]]) = }') # 키가 70inch인 사람의 몸무게 예측
```

    line_fitter.predict([[70]]) = array([133.26760811])
    


```python
print(f'{line_fitter.coef_ =}') # 기울기
print(f'{line_fitter.intercept_=}') # y절편
```

    line_fitter.coef_ =array([3.08347645])
    line_fitter.intercept_=-82.57574306454079
    

<br>
간단한 다중 선형 회귀 모델의 예제이다. <br>

```python
import pandas as pd

df = pd.read_csv("manhattan.csv") # kaggle의 Rent Amount of June 2016 Streeteasy 데이터
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rental_id</th>
      <th>building_id</th>
      <th>rent</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>size_sqft</th>
      <th>min_to_subway</th>
      <th>floor</th>
      <th>building_age_yrs</th>
      <th>no_fee</th>
      <th>has_roofdeck</th>
      <th>has_washer_dryer</th>
      <th>has_doorman</th>
      <th>has_elevator</th>
      <th>has_dishwasher</th>
      <th>has_patio</th>
      <th>has_gym</th>
      <th>neighborhood</th>
      <th>submarket</th>
      <th>borough</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1545</td>
      <td>44518357</td>
      <td>2550</td>
      <td>0.0</td>
      <td>1</td>
      <td>480</td>
      <td>9</td>
      <td>2.0</td>
      <td>17</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>Upper East Side</td>
      <td>All Upper East Side</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2472</td>
      <td>94441623</td>
      <td>11500</td>
      <td>2.0</td>
      <td>2</td>
      <td>2000</td>
      <td>4</td>
      <td>1.0</td>
      <td>96</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Greenwich Village</td>
      <td>All Downtown</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10234</td>
      <td>87632265</td>
      <td>3000</td>
      <td>3.0</td>
      <td>1</td>
      <td>1000</td>
      <td>4</td>
      <td>1.0</td>
      <td>106</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Astoria</td>
      <td>Northwest Queens</td>
      <td>Queens</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2919</td>
      <td>76909719</td>
      <td>4500</td>
      <td>1.0</td>
      <td>1</td>
      <td>916</td>
      <td>2</td>
      <td>51.0</td>
      <td>29</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>Midtown</td>
      <td>All Midtown</td>
      <td>Manhattan</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2790</td>
      <td>92953520</td>
      <td>4795</td>
      <td>1.0</td>
      <td>1</td>
      <td>975</td>
      <td>3</td>
      <td>8.0</td>
      <td>31</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>Greenwich Village</td>
      <td>All Downtown</td>
      <td>Manhattan</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
mlr = LinearRegression()
mlr.fit(x_train, y_train) 
```




<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>




```python
my_apartment = [[1, 1, 620, 16, 1, 98, 1, 0, 1, 0, 0, 1, 1, 0]]
my_predict = mlr.predict(my_apartment)
print(f'{my_predict=}')
```

    my_predict=array([[2005.32275882]])
    

   
    


```python
print(mlr.coef_) #계수 확인
```

    [[-486.60032808 1389.89343765    4.69852898  -17.65217356   39.85448773
        -4.11190558  -85.95343096   39.9080553   141.21411307   10.50324996
       164.47431448   55.05079097 -194.76346515    8.9496282 ]]
    


```python
# 일치하는지 확인 :완전한 직선이면 일치(실제-예측 그래프)
import matplotlib.pyplot as plt
y_predict = mlr.predict(x_test)
plt.scatter(y_test, y_predict, alpha=0.4)
plt.xlabel("Actual Rent")
plt.ylabel("Predicted Rent")
plt.title("MULTIPLE LINEAR REGRESSION")
plt.show()
```


    
![png](https://drive.google.com/uc?id=1bIeQWnQdJHjHJjXTKn2vYRDIa9EjSwMz)
    



```python
print(mlr.score(x_train, y_train)) #결정계수
```

    0.7373487000930192
    

<br>
<br>
#### 2-2-2. Decision Tree(분류/회귀)
- Decision Tree(의사결정 나무)는 각 데이터들이 가진 속성들로부터 패턴을 찾아내서 분류과제를 수행할 수 있도록 하는 지도학습 머신러닝 모델이다.<br>
![png](https://drive.google.com/uc?id=1IxF52d6771PCT4eLU4ctkFa9LtpOhkH-) <br>
*출처: https://ratsgo.github.io/machine%20learning/2017/03/26/tree/* <br>
<br>

**Decision Tree 만드는 방법**<br>
- 모든 데이터를 포함한 하나의 노드로 구성된 트리에서 시작한다.<br>
<br>
분할속성을 선택 -> 속성값에 따라 서브트리(자식)을 생성-> 데이터를 속성값에 따라 분배하며 노드 분할을 한다.<br>
분할 속성을 결정할때에는 엔트로피를 낮추는 쪽으로 분할한다.<br>
엔트로피(Entropy)는 불확실성을 뜻한다. 섞인 정도가 클수록 큰 값이다.
 m개의 레코드가 속하는 영역에 대한 엔트로피는 다음과 같다.<br>
$-\sum_{k=1}^{m} p_k \log_2 (p_k)$<br>
<br>
- 정보이득(IG) $= I - I_{res} $<br>

$I_{res} $는 특정 속성으로 분할한 후의 각 부분집합의 정보량의 가중평균을 의미한다.<br>
$I_{res} = -\sum_{i=1}^{d} p_i (\sum_{k=1}^{m} p_k \log_2 (p_k))$<br>
정보이득이 클수록 우수한 분할 속성이다.<br>
그러나 정보이득 척도는 속성값이 많으면 데이터 집합을 많은 부분집합으로 분할하는 등의 단점이 있다.<br>
이를 개선하기 위한 척도에 정보이득비와 지니 지수 등이 있다.<br>
<br>
- 정보이득비(information gain ratio)<br>

A는 속성을 의미한다.<br>
$GainRatio(A) = \frac{IG(A)}{I(A)}$<br>
$I(A)$도 속성 A의 속성값을 부류로 간주하여 계산한 엔트로피를 뜻한다.
$I(A) = $-\sum_{k=1}^{n} p_k \log_2 (p_k)$ -> 식은 비슷하지만, 한 속성을 부류로 간주하여 계산(n은 한 속성의 레코드의 수)<br>
<br>
- 지니지수(Gini index) $GINI(A)$ <br>

$GINI(A) = \sum_{i=1}^{d} (R_i(1-(\sum_{k=1}^{m} p_{ik} ^2)))$
<Br>
<br>

분류와 회귀 모두 가능하다.<br>

**분류를 위한 의사결정 나무:** <br>
- 새로운 데이터가 특정 노드에 속한다는 정보를 확인하고 그 노드에서 가장 빈도가 높은 범주에 데이터를 분류한다.<br>
- 분류를 할때에는 속성이 수치가 아닌 범주로 나와있다.<br>
<br>

scikit-learn에 나와있는 의사결정나무를 이용한 분류의 예제이다. iris데이터를 이용한 의사결정나무의 그림까지 보여준다.
<br>
```python
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
```


```python
clf.predict([[2., 2.]])
```




    array([1])




```python
#iris 데이터셋에서 확인
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
X, y = iris.data, iris.target
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, y)
```


```python
tree.plot_tree(clf)
```




    [Text(0.5, 0.9166666666666666, 'x[2] <= 2.45\ngini = 0.667\nsamples = 150\nvalue = [50, 50, 50]'),
     Text(0.4230769230769231, 0.75, 'gini = 0.0\nsamples = 50\nvalue = [50, 0, 0]'),
     Text(0.5769230769230769, 0.75, 'x[3] <= 1.75\ngini = 0.5\nsamples = 100\nvalue = [0, 50, 50]'),
     Text(0.3076923076923077, 0.5833333333333334, 'x[2] <= 4.95\ngini = 0.168\nsamples = 54\nvalue = [0, 49, 5]'),
     Text(0.15384615384615385, 0.4166666666666667, 'x[3] <= 1.65\ngini = 0.041\nsamples = 48\nvalue = [0, 47, 1]'),
     Text(0.07692307692307693, 0.25, 'gini = 0.0\nsamples = 47\nvalue = [0, 47, 0]'),
     Text(0.23076923076923078, 0.25, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]'),
     Text(0.46153846153846156, 0.4166666666666667, 'x[3] <= 1.55\ngini = 0.444\nsamples = 6\nvalue = [0, 2, 4]'),
     Text(0.38461538461538464, 0.25, 'gini = 0.0\nsamples = 3\nvalue = [0, 0, 3]'),
     Text(0.5384615384615384, 0.25, 'x[2] <= 5.45\ngini = 0.444\nsamples = 3\nvalue = [0, 2, 1]'),
     Text(0.46153846153846156, 0.08333333333333333, 'gini = 0.0\nsamples = 2\nvalue = [0, 2, 0]'),
     Text(0.6153846153846154, 0.08333333333333333, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]'),
     Text(0.8461538461538461, 0.5833333333333334, 'x[2] <= 4.85\ngini = 0.043\nsamples = 46\nvalue = [0, 1, 45]'),
     Text(0.7692307692307693, 0.4166666666666667, 'x[0] <= 5.95\ngini = 0.444\nsamples = 3\nvalue = [0, 1, 2]'),
     Text(0.6923076923076923, 0.25, 'gini = 0.0\nsamples = 1\nvalue = [0, 1, 0]'),
     Text(0.8461538461538461, 0.25, 'gini = 0.0\nsamples = 2\nvalue = [0, 0, 2]'),
     Text(0.9230769230769231, 0.4166666666666667, 'gini = 0.0\nsamples = 43\nvalue = [0, 0, 43]')]




    
![png](https://drive.google.com/uc?id=1Ku9-dlmXVwiEtD8HJVw9KL6WFALvNao9)
    

<br>
<br>

**회귀를 위한 의사결정 나무:** <br>
- 회귀를 할때에는 출력값이 수치이다.<br>
- 새로운 데이터가 특정 노드에 속한다는 정보를 확인하고 해당 노드의 종속변수의 평균을 예측값으로 반환한다.<br>
- 분할속성을 선택할때에도 표준편차축소를 최대로 하는 속성( = 표준편차가 작아지는 속성)을 선택한다.<br>
- $SDR(A) = SD-SD(A)$<br>

$SD(A)$는 속성을 A를 기준으로 분할 후의 부분 집합별 표준편차의 가중평균을 의미한다.<br>
->계산결과 SDR이 크게하는 조건을 분할로 잡는다.<br>
<br>
scikit-learn에 나와있는 아주 간단한 의사결정나무를 이용한 회귀의 예제이다. <br>
```python
from sklearn import tree
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
clf = tree.DecisionTreeRegressor()
clf = clf.fit(X, y)
clf.predict([[1, 1]])
```




    array([0.5])


<br>

#### 2-2-3. Random Forests
앙상블 분류기<br>
-주어진 학습 데이터 집합에 대해 여러개의 서로 다른 분류기를 만들고, 이들 분류기의 판정 결과를 투표 방식이나 가중치 투표방식으로 결합<br>
<br>

**붓스트랩(bootstrap)과 배깅(bagging)**<br>
- 붓스트랩은 주어진 학습 데이터 집합에서 복원추출하여 다수의 학습 데이터 집합을 만들어내는 기법을 말한다.<br>
- 중복을 허용한다(중복을 허용하지 않는 샘플링은 페이스팅)<br>
<br>
- bagging은 bootstrap aggregating의 줄임말로,<br>

붓스트랩을 통해 여러개의 학습 데이터 집합을 만들고 각 학습데이터 집합별로 분류기를 만들어, 투표나 가중치 투표를 통해 최종 판정을 하는 기법이다.<br>
![png](https://drive.google.com/uc?id=10HUBm65X9xPVWGk8oJswnKIOoFn88U7N)<br>
*출처: https://fouaaa.blogspot.com/2019/11/blog-post_20.html* <br>
<br>
- 랜덤 포레스트 알고리즘은 분류기로 결정트리를 사용하는 배깅 기법이다.<br>
<br>

scikit-learn에 나와있는 랜덤포레스트 기법의 예제이다.<br>
n-estimator로 나무의 개수를 정하는 것 말고는 의사결정 나무와 거의 비슷하다.<Br>

```python
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier # extremely randomized trees
from sklearn.tree import DecisionTreeClassifier

X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
    random_state=0)

clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
    random_state=0)
scores = cross_val_score(clf, X, y, cv=5)
scores.mean()
```




    0.9823000000000001




```python
clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0) #n-estimators:나무의 개수
scores = cross_val_score(clf, X, y, cv=5)
scores.mean()
```




    0.9997




```python
clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
min_samples_split=2, random_state=0)
scores = cross_val_score(clf, X, y, cv=5)
scores.mean()
```




    1.0



