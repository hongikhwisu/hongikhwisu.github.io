---
layout : single
title:  비지도학습:클러스터링
use_math: true
categories: Unsupervised_learning
---
# 1. 비지도학습
- 사람의 도움없이 컴퓨터가 스스로 데이터를 파악하고 규칙을 찾는 방법을 말한다.<br>
-> y값 없이 x값만으로 데이터를 학습하는 방법<br>
- 비지도학습 시에는 비정형 데이터를 학습한다.<br>
- 훈련 정보가 사전에 정답이 레이블 되지 않거나, 구조를 알 수 없는 데이터일때 그 데이터의 의미있는 정보를 추출한다.<br>
데이터세트에서 추론을 도출하고, 레이블이 없는 데이터에 있을 수 있는 숨겨진 구조를 표시한다.<br>

<br>
# 2. 여러가지 비지도학습 알고리즘들
- 비지도학습의 알고리즘은 군집화(clustering), 차원 축소(dimensionality reduction), 연관성 분석(association analysis)에 사용된다.<br>
- Catgorical 데이터에는 연관성 분석, Hidden Markov model이 사용되고, Continuous 데이터에는 군집화와 차원축소가 사용된다.<br>
- 차원축소에는 SVD, PCA등이 사용되고, 군집화에는 K-means, DBSCAN등이 사용된다.<br>
- 연관성분석에는 APriori, FP-Growth등이 사용된다.<br>

<br>
## 2-1. 군집화(clustering)
- 군집화의 목표는 집단내 동질성을 최대화, 집단간 이질성을 최대화 하는 것이다.<br>
-> 그룹 내의 객체들은 유사하도록 군집 내 중심과 해당 군집의 각 개체간 거리의 합을 최소화<br>
-> 그룹간의 객체들은 유사하지 않도록 각 군집의 중심간 거리 합 최대화<br>
<br>

**유사도 측정법(거리)**<br>
![png](https://drive.google.com/uc?id=1lYUA9cLt08luoCCFSH2pphzzOpPT_C20) <br>
*출처: https://tuhinmukherjee74.medium.com/*<br>
그림에 나와있는 것 말고도 여러가지 측정방법이 있다. 대표적인 것만 알아보면<br>
<br>

1. 유클리디안 거리
- n차원에서도 사용 가능한, 두 점 사이의 직선거리를 계산하는 방법<br>
$\sqrt{\sum _{i=1}^{k} (x_i - y_i)^2}$<br>
<br>

2. 맨하탄 거리<br>
- 격자로 이루어진 공간에서 두 점 사이의 수직/수평 거리의 합을 계산하는 방법<br>
- 주로 2차원이나 3차원에서 사용됨<br>
$\sum _{i=1}^{k} |x_i - y_i|$<br>
<br>

3. 민코프스키 거리<br>
- 유클리드 거리와 맨해튼 거리를 일반화<br>
- p가 1일 경우 맨해튼 거리와 동일, p가 2일 경우 유클리드 거리와 동일<br>
$(\sum_{i=1}^{k} |x_i-y_i|^p)^{\frac{1}{p}}$<br>
<br>

4. 코사인 유사도<br>
- 두 벡터가 이루는 각도를 통해 유사도를 측정하는 <br>
- 두 벡터가 이루는 각이 작을수록 유사도가 높음(각이 0도면 유사도가 1, 각이 180도면 유사도가 -1에 가까움)<br>
-> 벡터의 크기를 고려하고 싶지 않을때 사용<br>
$cos(\theta) = \frac{A\cdot B}{||A||\,||B||} =\frac {\sum_{i=1}^{n}A_i B_i}{\sqrt{\sum _{i=1}^{n} A^2 _i}\,\sqrt{\sum _{i=1}^{n} B^2 _i}}$<br>
<br>

**분류와 군집화의 차이점**<br>
- 분류는 데이터의 레이블들이 미리 정해져 있지만 군집화는 데이터의 레이블들이 정해져 있지 않다.<br>
- 분류는 알고있는 여러개의 클래스로 데이터를 말 그대로 분류하는 것이고, 군집화는 유사한 특성들 가진 데이터를 같은 그룹으로 묶는 이다.<br>

#### 2-1-1. K-means 클러스터링
- K는 데이터 세트에서 찾을 것으로 예상되는 그룹의 개수를 뜻한다.<br>
- Means는 각 데이터로부터 그 데이터가 속한 클러스터의 중심까지의 평균 거리를 의미한다.<br>
->이 값을 최소화하는 게 알고리즘의 목표가 된다.<br>
- EDA단계에서 부터 적용할 수 있다.(광범위하게 사용)<br>
<br>

**K-means 클러스터링 알고리즘** <br>
1. 군집의 개수(K)를 정한다.<br>
2. 결정한 군집의 개수만큼(=K개의) 데이터 중에 임의의 중심값(centroid)를 정한다.<br>
3. 각 데이터 마다 가까운 중심값을 정하여 데이터들을 중심값에 할당한다.<br>
4. 군집내 할당된 데이터들을 이용해 평균값을 계산하고 이를 새로운 중심값으로 정한다.<br>
5. 3번과 4번을 반복하면서 데이터의 중심값이 바뀌지 않을 때까지 계속한다.<br>

- K-means 클러스터링은 초기중심점으로 어떤 값을 선택하는가에 따라 성능이 크게 달라진다.<br>
따라서 초기 중심값을 설정할때는 잘 설정해야 하고, 여러가지 방법이 있다.<br>
<br>

i)무작위 분할<br>
- 가장 많이 사용되는 초기화 기법이다.<br>
- 각 데이터들을 임의의 클러스터에 배당하고, 그 배당된 점들의 평균값을 초기값으로 선정한다.<br>
<br>

ii) K-means++ 초기화<br>
- 무작위 분할의 문제점을 개선하기 위해 제안된 방법중 하나이다.<br>
1. 첫번째 중심을 무작위로 정하는 것은 같지만, 무작위로 정한 뒤 나머지 점들과의 거리를 각각 계산한다.<br>
2. 거리를 계산하고, 가장 멀리있는 점을 다음 중심점으로 지정한다.<br>
3. 중심점이 K개가 될 때까지 1,2번을 반복한다.<br>
<br>

k의 수를 정하는 법<br>
1. Rules of thumb<br>
-가장 간단한 방법<br>
-데이터수가 n이라 할때 필요한 클러스터의 수는 $\sqrt {\frac{n}{2}}$<br>
<br>

2. Silhouette method<br>
-실루엣 지수($S = \frac{b-a}{max(a,b)}, S \in [-1,1]$) 사용<br>
a는 동일한 클러스터내의 다른 모든 데이터포인트까지의 평균 거리,<br>
b는 가장 가까운 다른 클러스터까지의 평균 거리를 뜻한다.<br>
-클러스터내 데이터 응집도가 최대, 클러스터간 분리도가 최대가 되는 k값을 구하는 방법이고, k가 최적화 될수록 값이 1에 가까워진다.<br>
<br>
3. elbow method<br>
-클러스터 내부 분산의 총량(SSE)를 계산한다.(k = 1부터 시작하여 점점 값을 늘려가면서 계산)<br>
->k값이 증가함에 따라 분산이 가장 급격하게 감소하는 지점의 k를 최종 군집의 개수로 정한다.<br>
<br>

**k-means 클러스터링의 장단점**<br>
장점<br>
- 쉽고 빠르게 연산이 가능하다.<br>
- 데이터에 대한 사전 정보 없이도 효과적으로 데이터 구조를 탐색한다.<br>
<br>

단점<br>
- k값을 사전에 정해줘야하고, k값의 영향을 많이 받는다.<br>
- 이상치에 민감하게 반응한다.<br>
- 평균을 취하므로 범주형 데이터에는 적합하지 않다.<br>
(평균값대신 중앙값을 취하면 이상치의 영향을 덜 받고, 범주형 데이터에도 적용 가능하다:k-medoid clustering)<br>
<br>

k-means 클러스터링의 예제이다.<br>
kaggle의 Mall Customer Segmentation Data를 이용했고, k를 3~6까지 지정한 결과이다.<br>

```python
import pandas as pd
df = pd.read_csv('Mall_Customers.csv')
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>195</th>
      <td>196</td>
      <td>Female</td>
      <td>35</td>
      <td>120</td>
      <td>79</td>
    </tr>
    <tr>
      <th>196</th>
      <td>197</td>
      <td>Female</td>
      <td>45</td>
      <td>126</td>
      <td>28</td>
    </tr>
    <tr>
      <th>197</th>
      <td>198</td>
      <td>Male</td>
      <td>32</td>
      <td>126</td>
      <td>74</td>
    </tr>
    <tr>
      <th>198</th>
      <td>199</td>
      <td>Male</td>
      <td>32</td>
      <td>137</td>
      <td>18</td>
    </tr>
    <tr>
      <th>199</th>
      <td>200</td>
      <td>Male</td>
      <td>30</td>
      <td>137</td>
      <td>83</td>
    </tr>
  </tbody>
</table>
<p>200 rows × 5 columns</p>
</div>




```python
from sklearn.preprocessing import MinMaxScaler #데이터전처리: MinMaxscaler사용
data = df[['Annual Income (k$)','Spending Score (1-100)']]
scaler = MinMaxScaler()
data_scale = scaler.fit_transform(data)
```


```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
k = 3

model = KMeans(n_clusters = k, random_state = 10)
model.fit(data_scale)
df['cluster'] = model.fit_predict(data_scale)

plt.figure(figsize = (8, 8))
for i in range(k):
    plt.scatter(df.loc[df['cluster'] == i, 'Annual Income (k$)'], df.loc[df['cluster'] == i, 'Spending Score (1-100)'], label = 'cluster ' + str(i))
plt.legend()
plt.title('K = %d results'%k , size = 15)
plt.xlabel('Annual Income', size = 12)
plt.ylabel('Spending Score', size = 12)
plt.show()
```
    
![png](https://drive.google.com/uc?id=1Uw838s2hdns7O4Fg_Fx69-znZ41bQqPN)
    



```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
k = 4

model = KMeans(n_clusters = k, random_state = 10)
model.fit(data_scale)
df['cluster'] = model.fit_predict(data_scale)

plt.figure(figsize = (8, 8))
for i in range(k):
    plt.scatter(df.loc[df['cluster'] == i, 'Annual Income (k$)'], df.loc[df['cluster'] == i, 'Spending Score (1-100)'], label = 'cluster ' + str(i))
plt.legend()
plt.title('K = %d results'%k , size = 15)
plt.xlabel('Annual Income', size = 12)
plt.ylabel('Spending Score', size = 12)
plt.show()
```


    
![png](https://drive.google.com/uc?id=1Ku9-dlmXVwiEtD8HJVw9KL6WFALvNao9)
    



```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
k = 5

model = KMeans(n_clusters = k, random_state = 10)
model.fit(data_scale)
df['cluster'] = model.fit_predict(data_scale)

plt.figure(figsize = (8, 8))
for i in range(k):
    plt.scatter(df.loc[df['cluster'] == i, 'Annual Income (k$)'], df.loc[df['cluster'] == i, 'Spending Score (1-100)'], label = 'cluster ' + str(i))
plt.legend()
plt.title('K = %d results'%k , size = 15)
plt.xlabel('Annual Income', size = 12)
plt.ylabel('Spending Score', size = 12)
plt.show()
```


![png](https://drive.google.com/uc?id=1ESwoTqdFNj297ID6blkumcAQvq8KQGxN)
    



```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
k = 6

model = KMeans(n_clusters = k, random_state = 10)
model.fit(data_scale)
df['cluster'] = model.fit_predict(data_scale)

plt.figure(figsize = (8, 8))
for i in range(k):
    plt.scatter(df.loc[df['cluster'] == i, 'Annual Income (k$)'], df.loc[df['cluster'] == i, 'Spending Score (1-100)'], label = 'cluster ' + str(i))
plt.legend()
plt.title('K = %d results'%k , size = 15)
plt.xlabel('Annual Income', size = 12)
plt.ylabel('Spending Score', size = 12)
plt.show()
```

  
![png](https://drive.google.com/uc?id=1v7e5UZCpR1AWG0lFgpbIgr5eDt6FmLlK)
    

<br>
k=5일때 가장 분류가 잘 된다.
<br>
<br>
#### 2-1-2. DBSCAN
- Density-Based Spatial Clustering of Applications(노이즈를 적용한 밀도 기반 공간 클러스터링)의 줄임말이다.<br>
- 점이 세밀하게 몰려있어 밀도가 높은 부분을 클러스터링 하는 방식이다.<br>
- 계산 할때에는 거리(e)과 최소요소(M)을 사용하고, 데이터세트의 요소는 core/border/outlier point로 구분된다.<br>
<br>

**DBSCAN 알고리즘**<br>
- 거리 e(epsilon)안에 점이 M개 있으면 하나의 군집으로 인식한다.<br>
- 자신으로 부터 반경 e안에 점 M개 이상 있는 점은 core point가 된다.<br>
- core point에 속하는 점은 border point라고 한다.<br>
- 어느 점을 중심으로 해도 주변에 M개 이상을 만족 시키는 범위에 포함되어 있지 않으면 noise point라고 한다.<br>

core point의 반경 e안에 다른 core point가 있으면 연결되어 있다고 하고 하나의 군집으로 묶인다.<br>
<br>

![png](https://drive.google.com/uc?id=1MOi2woTqgeiCLdWdtx99eTEdwaP1i-RU)<br>
*출처: https://bcho.tistory.com/1205*<br>
noise point를 통해 outlier검출이 가능하고, k의 개수를 미리 정할 필요도 없고, 기하학적 모양을 갖는 군집도 찾을 수 있는 등의 장점이 있다.<br>
<br>
scikit-learn에 있는 DBSCAN 예제이다.<br>

```python
#데이터를 형성하는 과정
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(
    n_samples=750, centers=centers, cluster_std=0.4, random_state=0
)

X = StandardScaler().fit_transform(X)

plt.scatter(X[:, 0], X[:, 1])
plt.show()
```


    
![png](https://drive.google.com/uc?id=1xgoZzwi9yQbxggdtS0_p3pEpRoGn1yfJ)
    



```python
import numpy as np

from sklearn import metrics
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.3, min_samples=10).fit(X) #거리 e와 최소 샘플 개수 M설정
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print("Estimated number of clusters: %d" % n_clusters_) #클러스터 수
print("Estimated number of noise points: %d" % n_noise_) # noise point의 개수
```

    Estimated number of clusters: 3
    Estimated number of noise points: 18

<br>

*출처:* <br>
*2학년 1학기 데이터사이언스 자료* <br>
*https://techscene.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9C%A0%EC%82%AC%EB%8F%84-%EB%B0%8F-%EA%B1%B0%EB%A6%AC-%EC%B4%9D%EC%A0%95%EB%A6%AC-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88-%EC%9E%90%EC%B9%B4%EB%93%9C-%EB%A9%98%ED%95%98%ED%83%84-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98* <br>
*https://tuhinmukherjee74.medium.com/* <br>
*https://velog.io/@jhlee508/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-K-%ED%8F%89%EA%B7%A0K-Means-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98*<br>
*https://jimmy-ai.tistory.com/52* <br>
*https://bcho.tistory.com/1205* <br>
*https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py*

